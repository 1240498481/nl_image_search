{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nl_image_search.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPzqGUN/yZr1/PKR8pOlVc4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ix2Tks6qePJt"},"source":["!pip install -q -U tensorflow-hub tensorflow-text tensorflow-addons"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t4kifcFUfTab"},"source":["import os\n","import collections\n","import json\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras \n","from tensorflow.keras import layers\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","import tensorflow_addons as tfa\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from tqdm import tqdm\n","\n","tf.get_logger().setLevel('ERROR')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jx5WNV4Dfv95","executionInfo":{"status":"ok","timestamp":1622438536685,"user_tz":-480,"elapsed":3193,"user":{"displayName":"kai Ma","photoUrl":"","userId":"10828033128946369206"}},"outputId":"5cf6db2f-abdb-4190-f321-f38a7cb42ab3"},"source":["root_dir = 'datasets'\n","annotations_dir = os.path.join(root_dir, 'annotations')\n","images_dir = os.path.join(root_dir, 'train2014')\n","tfrecords_dir = os.path.join(root_dir, 'tfrecords')\n","annotation_file = os.path.join(annotations_dir, 'captions_train2014.json')\n","if not os.path.exists(annotations_dir):\n","  annotation_zip = tf.keras.utils.get_file(\n","      'captions.zip',\n","      cache_dir = os.path.abspath('.'),\n","      origin=\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\",\n","      extract=True,\n","  )\n","  os.remove(annotation_zip)\n","\n","if not os.path.exists(images_dir):\n","  image_zip = tf.keras.utils.get_file(\n","      \"train2014.zip\",\n","      cache_dir=os.path.abspath(\".\"),\n","      origin=\"http://images.cocodataset.org/zips/train2014.zip\",\n","      extract=True, \n","  )\n","  os.remove(image_zip)\n","\n","print('Dataset is download and extracted successfully')\n","\n","with open(annotation_file, 'r') as f:\n","  annotations = json.load(f)['annotations']\n","\n","image_path_to_caption = collections.defaultdict(list)\n","for element in annotations:\n","  caption = f\"{element['caption'].lower().rstrip('.')}\"\n","  image_path = images_dir + \"/COCO_train2014_\" + \"%012d.jpg\" % (element['image_id'])\n","  image_path_to_caption[image_path].append(caption)\n","\n","image_paths = list(image_path_to_caption.keys())\n","print(f\"Number of images: {len(image_paths)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dataset is download and extracted successfully\n","Number of images: 82783\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVZwir7-jwNk","executionInfo":{"status":"ok","timestamp":1622438798062,"user_tz":-480,"elapsed":261383,"user":{"displayName":"kai Ma","photoUrl":"","userId":"10828033128946369206"}},"outputId":"5f959ce7-1036-4377-e153-3d6fd739f5b2"},"source":["train_size = 30000\n","valid_size = 5000\n","captions_per_image = 2\n","images_per_file = 2000\n","\n","train_image_paths = image_paths[: train_size]\n","num_train_files = int(np.ceil(train_size / images_per_file))\n","train_files_prefix = os.path.join(tfrecords_dir, 'train')\n","\n","valid_image_paths = image_paths[-valid_size:]\n","num_valid_files = int(np.ceil(valid_size / images_per_file))\n","valid_files_prefix = os.path.join(tfrecords_dir, 'valid')\n","\n","tf.io.gfile.makedirs(tfrecords_dir)\n","\n","def bytes_feature(value):\n","  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n","\n","def create_example(image_path, caption):\n","  feature = {\n","      'caption': bytes_feature(caption.encode()),\n","      'raw_image': bytes_feature(tf.io.read_file(image_path).numpy())\n","  }\n","  return tf.train.Example(features=tf.train.Features(feature=feature))\n","\n","def write_tfrecords(file_name, image_paths):\n","  caption_list = []\n","  image_path_list = []\n","  for image_path in image_paths:\n","    captions = image_path_to_caption[image_path][:captions_per_image]\n","    caption_list.extend(captions)\n","    image_path_list.extend([image_path] * len(captions))\n","\n","  with tf.io.TFRecordWriter(file_name) as writer:\n","    for example_idx in range(len(image_path_list)):\n","      example = create_example(\n","          image_path_list[example_idx], caption_list[example_idx]\n","      )\n","      writer.write(example.SerializeToString())\n","  return example_idx + 1\n","\n","def write_data(image_paths, num_files, files_prefix):\n","  example_counter = 0\n","  for file_idx in tqdm(range(num_files)):\n","    file_name = files_prefix + \"-%02d.tfrecord\" % (file_idx)\n","    start_idx = images_per_file * file_idx\n","    end_idx = start_idx + images_per_file\n","    example_counter += write_tfrecords(file_name, image_paths[start_idx:end_idx])\n","  return example_counter\n","\n","train_example_count = write_data(train_image_paths, num_train_files, train_files_prefix)\n","print(f\"{train_example_count} training examples were written to tfrecord files.\")\n","\n","valid_example_count = write_data(valid_image_paths, num_valid_files, valid_files_prefix)\n","print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 15/15 [03:42<00:00, 14.81s/it]\n","  0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["60000 training examples were written to tfrecord files.\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3/3 [00:39<00:00, 13.05s/it]"],"name":"stderr"},{"output_type":"stream","text":["10000 evaluation examples were written to tfrecord files.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"pnx6i1GbmV0X"},"source":["feature_description = {\n","    'caption': tf.io.FixedLenFeature([], tf.string),\n","    'raw_image': tf.io.FixedLenFeature([], tf.string)\n","}\n","\n","def read_example(example):\n","  features = tf.io.parse_single_example(example, feature_description)\n","  raw_image = features.pop('raw_image')\n","  features['image'] = tf.image.resize(\n","      tf.image.decode_jpeg(raw_image, channels=3), size=(299, 299)\n","  )\n","  return features\n","\n","def get_dataset(file_pattern, batch_size):\n","  return (\n","      tf.data.TFRecordDataset(tf.data.Dataset.list_files(file_pattern)).map(\n","          read_example,\n","          num_parallel_calls=tf.data.experimental.AUTOTUNE,\n","          deterministic=False\n","      )\n","      .shuffle(batch_size * 10)\n","      .prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n","      .batch(batch_size)\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eEoBTrNHpcqD"},"source":["def project_embeddings(\n","    embeddings, num_projection_layers, projection_dims, dropout_rate\n","):\n","  projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n","  for _ in range(num_projection_layers):\n","    x = tf.nn.gelu(projected_embeddings)\n","    x = layers.Dense(projection_dims)(x)\n","    x = layers.Dropout(dropout_rate)(x)\n","    x = layers.Add()([projected_embeddings, x])\n","    projected_embeddings = layers.LayerNormalization()(x)\n","  return projected_embeddings"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lXFjpJ4tB0D"},"source":["def create_vision_encoder(\n","    num_projection_layers, projection_dims, dropout_rate, trainable=False\n","):\n","  xception = keras.applications.Xception(\n","      include_top = False,\n","      weights='imagenet',\n","      pooling='avg'\n","  )\n","  for layer in xception.layers:\n","    layer.trainable = trainable\n","  inputs = layers.Input(shape=(299, 299, 3), name='image_input')\n","  xception_input = tf.keras.applications.xception.preprocess_input(inputs)\n","  embeddings = xception(xception_input)\n","  outputs = project_embeddings(\n","      embeddings, num_projection_layers, projection_dims, dropout_rate\n","  )\n","  return keras.Model(inputs, outputs, name='vision_encoder')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AbP1HBHkt6uw"},"source":["def create_text_encoder(\n","    num_projection_layers, projection_dims, dropout_rate, trainable=False\n","):\n","  preprocess = hub.KerasLayer(\n","      'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2', name='text_preprocessing'\n","  )\n","  bert = hub.KerasLayer(\n","      'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1', 'bert'\n","  )\n","  bert.trainable = trainable\n","  inputs = layers.Input(shape=(), dtype=tf.string, name='text_input')\n","  bert_inputs = preprocess(inputs)\n","  embeddings = bert(bert_inputs)['pooled_output']\n","  outputs = project_embeddings(\n","      embeddings, num_projection_layers, projection_dims, dropout_rate\n","  )\n","  return keras.Model(inputs, outputs, name='text_encoder')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7m7nhL4Au8YO"},"source":["class DualEncoder(keras.Model):\n","  def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs):\n","    super(DualEncoder, self).__init__(**kwargs)\n","    self.text_encoder = text_encoder\n","    self.image_encoder = image_encoder\n","    self.temperature = temperature\n","    self.loss_tracker = keras.metrics.Mean(name='loss')\n","\n","  @property\n","  def metrics(self):\n","    return [self.loss_tracker]\n","\n","  def call(self, features, training=False):\n","    with tf.device('/gpu:0'):\n","      caption_embeddings = text_encoder(features['caption'], training=training)\n","    with tf.device('/gpu:1'):\n","      image_embeddings = vision_encoder(features['image'], training=training)\n","    return caption_embeddings, image_embeddings\n","\n","  def compute_loss(self, caption_embeddings, image_embeddings):\n","    logits = (\n","        tf.matmul(caption_embeddings, image_embeddings, transpose_b = True) / self.temperature\n","    )\n","    images_similarity = tf.matmul(\n","        image_embeddings, image_embeddings, transpose_b = True\n","    )\n","    captions_similarity = tf.matmul(\n","        caption_embeddings, caption_embeddings, transpose_b = True\n","    )\n","    targets = keras.activations.softmax(\n","        (captions_similarity + images_similarity) / (2 * self.temperature)\n","    )\n","    captions_loss = keras.losses.categorical_crossentropy(\n","        y_true = targets,\n","        y_pred = logits,\n","        from_logits=True\n","    )\n","    images_loss = keras.losses.categorical_crossentropy(\n","        y_true = tf.transpose(targets),\n","        y_pred = tf.transpose(logits),\n","        from_logits=True\n","    )\n","    return (captions_loss + images_loss) / 2\n","\n","  def train_step(self, features):\n","    with tf.GradientTape() as tape:\n","      caption_embeddings, image_embeddings = self(features, training=True)\n","      loss = self.compute_loss(caption_embeddings, image_embeddings)\n","    gradients = tape.gradient(loss, self.trainable_variables)\n","    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","    self.loss_tracker.update_state(loss)\n","    return {'loss': self.loss_tracker.result()}\n","\n","  def test_step(self, features):\n","    caption_embeddings, image_embeddings = self(features, training=False)\n","    loss = self.compute_loss(caption_embeddings, image_embeddings)\n","    self.loss_tracker.update_state(loss)\n","    return {'loss': self.loss_tracker.result()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cBaAf9YJxutq"},"source":["num_epochs = 5\n","batch_size = 256\n","\n","vision_encoder = create_vision_encoder(\n","    num_projection_layers=1,\n","    projection_dims=256,\n","    dropout_rate=0.1\n",")\n","text_encoder = create_text_encoder(\n","    num_projection_layers=1,\n","    projection_dims=256,\n","    dropout_rate = 0.1\n",")\n","dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)\n","dual_encoder.compile(\n","    optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGcaRJpUyYHl","executionInfo":{"status":"ok","timestamp":1622445460269,"user_tz":-480,"elapsed":6484385,"user":{"displayName":"kai Ma","photoUrl":"","userId":"10828033128946369206"}},"outputId":"1e22c5c0-1b72-41fc-e8b8-e74f9c0064f6"},"source":["print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n","print(f\"Number of examples (caption-image pairs): {train_example_count}\")\n","print(f\"Batch size: {batch_size}\")\n","print(f\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\")\n","train_dataset = get_dataset(os.path.join(tfrecords_dir, 'train-*.tfrecord'), batch_size)\n","valid_dataset = get_dataset(os.path.join(tfrecords_dir, 'valid-*.tfrecord'), batch_size)\n","reduce_lr = keras.callbacks.ReduceLROnPlateau(\n","    monitor='val_loss',\n","    factor=0.2,\n","    patience=3\n",")\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss', patience=5, restore_best_weights=True\n",")\n","history = dual_encoder.fit(\n","    train_dataset,\n","    epochs=num_epochs,\n","    validation_data=valid_dataset,\n","    callbacks=[reduce_lr, early_stopping]\n",")\n","print('Training completed. Saving vision and text encoders...')\n","vision_encoder.save('vision_encoder')\n","text_encoder.save('text_encoder')\n","print('Models ar saved.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of GPUs: 1\n","Number of examples (caption-image pairs): 60000\n","Batch size: 256\n","Steps per epoch: 235\n","Epoch 1/5\n","235/235 [==============================] - 1321s 5s/step - loss: 64.5259 - val_loss: 9.6705\n","Epoch 2/5\n","235/235 [==============================] - 1274s 5s/step - loss: 8.1765 - val_loss: 5.1975\n","Epoch 3/5\n","235/235 [==============================] - 1271s 5s/step - loss: 4.6813 - val_loss: 4.8003\n","Epoch 4/5\n","235/235 [==============================] - 1274s 5s/step - loss: 3.9209 - val_loss: 4.5409\n","Epoch 5/5\n","235/235 [==============================] - 1276s 5s/step - loss: 3.4715 - val_loss: 4.2076\n","Training completed. Saving vision and text encoders...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n","WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 310). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Models ar saved.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"W93sXggH0Pah","executionInfo":{"status":"ok","timestamp":1622446386419,"user_tz":-480,"elapsed":822,"user":{"displayName":"kai Ma","photoUrl":"","userId":"10828033128946369206"}},"outputId":"984e56e5-ef5a-4f7e-9b06-54875e30edcb"},"source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['train', 'valid'], loc='upper right')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"86A5Kra74Ee2","executionInfo":{"status":"ok","timestamp":1622447196817,"user_tz":-480,"elapsed":473572,"user":{"displayName":"kai Ma","photoUrl":"","userId":"10828033128946369206"}},"outputId":"f572e49a-d67d-4973-b24e-dbda28e6508e"},"source":["print('Loading vision and text encoders...')\n","vision_encoder = keras.models.load_model('vision_encoder')\n","text_encoder = keras.models.load_model('text_encoder')\n","print('Models are loaded.')\n","\n","def read_image(image_path):\n","  image_array = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)\n","  return tf.image.resize(image_array, (299, 299))\n","\n","print(f\"Generating embeddings for {len(image_paths)} images...\")\n","image_embeddings = vision_encoder.predict(\n","    tf.data.Dataset.from_tensor_slices(image_paths).map(read_image).batch(batch_size),\n","    verbose=1\n",")\n","print(f\"Image embeddings shape: {image_embeddings.shape}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading vision and text encoders...\n","Models are loaded.\n","Generating embeddings for 82783 images...\n","324/324 [==============================] - 705s 2s/step\n","Image embeddings shape: (82783, 256)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZTXVRV7u5p_4"},"source":["def find_matches(image_embeddings, queries, k=9, normalize=True):\n","  query_embedding = text_encoder(tf.convert_to_tensor(queries))\n","\n","  if normalize:\n","    image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n","    query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n","  \n","  dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b = True)\n","  results = tf.math.top_k(dot_similarity, k).indices.numpy()\n","  return [[image_paths[idx] for idx in indices] for indices in results]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcZ6CRMv6UtM"},"source":["query = 'a family standing next to the ocean on a sandy beach with a surf board'\n","matches = find_matches(image_embeddings, [query], normalize=True)[0]\n","\n","plt.figure(figsize=(20, 20))\n","for i in range(9):\n","  ax = plt.subplot(3, 3, i+1)\n","  plt.imshow(mping.imread(matches[i]))\n","  plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q7EKf1Hw_JXT"},"source":["def compute_top_k_accuracy(image_paths, k=100):\n","  hits = 0\n","  num_batches = int(np.ceil(len(image_paths) / batch_size))\n","  for idx in tqdm(range(num_batches)):\n","    start_idx = idx * batch_size\n","    end_idx = start_idx + batch_size\n","    current_image_paths = image_paths[start_idx: end_idx]\n","    queries = [\n","        image_path_to_caption[image_path][0] for image_path in current_image_paths\n","    ]\n","    result = find_matches(image_embeddings, queries, k)\n","    hits += sum(\n","        [\n","         image_path in matches\n","         for (image_path, matches) in list(zip(current_image_paths, result))\n","        ]\n","    )\n","  reulst hits / len(image_paths)\n","\n","print('Scoring training data ...')\n","train_accuracy = compute_top_k_accuracy(train_image_paths)\n","print(f\"Train accuracy: {round(train_accuracy * 100, 3)}%\")\n","\n","print(\"Scoring evaluation data...\")\n","eval_accuracy = compute_top_k_accuracy(image_paths[train_size: ])\n","print(f\"Eval accuracy: {round(eval_accuracy * 100, 3)}%\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2c-avcNcV03r"},"source":[""],"execution_count":null,"outputs":[]}]}